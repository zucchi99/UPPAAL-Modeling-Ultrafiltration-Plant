{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subfolder: UF/\n",
      "file_idx: 0\n",
      "tmp_idx: 0\n",
      "initial_feed_volume_L: 100\n",
      "reset_columns_when_OFF: True\n",
      "drop_initial_final_off_rows: True\n",
      "drop_off_rows: True\n",
      "drop_outliers: True\n",
      "plot_scatterplot_matrix: False\n",
      "use_default_arima_params: True\n",
      "default_arima_params: [1, 1, 0]\n",
      "include_arima_simulations_in_analysis: True\n"
     ]
    }
   ],
   "source": [
    "params = read_parameters()\n",
    "subfolder = params['subfolder']\n",
    "file_idx = params['file_idx']\n",
    "tmp_idx = params['tmp_idx']\n",
    "drop_initial_final_off_rows = params['drop_initial_final_off_rows']\n",
    "drop_off_rows = params['drop_off_rows']\n",
    "drop_outliers = params['drop_outliers']\n",
    "use_default_arima_params = params['use_default_arima_params']\n",
    "default_arima_params = params['default_arima_params']\n",
    "include_arima_simulations_in_analysis = params['include_arima_simulations_in_analysis']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_folder = PATH_SENSORS_DATA_EXT_UF_V1\n",
    "out_file = FILE_SENSORS_DATA_EST_PARAMS\n",
    "cur_file, file_path = get_input_file(in_folder=in_folder, in_file_idx=file_idx, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLUX_COL = 'flux at 20° [L/m^2h]' #'flux [L/m^2h]'\n",
    "FLUX_USED = FLUX_COL[:-9]\n",
    "print(FLUX_USED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMP_intervals = TMP_INTERVALS[cur_file]\n",
    "TMP_intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bckup = None\n",
    "df = pd.read_csv(file_path)\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "DATE = df.loc[0, 'datetime'].date().isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # already created\n",
    "    df_k_n = pd.read_csv(FILE_SENSORS_DATA_EST_PARAMS)\n",
    "except:\n",
    "    # to be created yet\n",
    "    df_k_n = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FILE_EST_COEFFS = '../data/estimated_coefficients.json'\n",
    "#conc_coeff_permeate  = read_estimated_coefficients_from_json(FILE_EST_COEFFS, 'concentration permeate')\n",
    "#conc_coeff_retentate = read_estimated_coefficients_from_json(FILE_EST_COEFFS, 'concentration retentate')\n",
    "#conc_coeff_extracted = {\n",
    "#    'retentate' : (conc_coeff_retentate['intercept'], conc_coeff_retentate['coefficients']),\n",
    "#    'permeate'  : (conc_coeff_permeate['intercept'],  conc_coeff_permeate['coefficients'])\n",
    "#}\n",
    "#initial_coeffs = get_initial_concentration_coefficients(df, conc_coeff_extracted)\n",
    "#initial_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis   = ('time [m]', 'datetime')\n",
    "x_format = (None,       '%H:%M')\n",
    "TIME_MINS = 0\n",
    "DATE_TIME = 1\n",
    "TIME_IDX  = TIME_MINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = drop_all_outliers(df, drop_initial_final_off_rows=drop_initial_final_off_rows, drop_off_rows=drop_off_rows, drop_outliers=drop_outliers, log=True)\n",
    "df = df.reset_index(drop=True)\n",
    "# reset time so it starts from 0\n",
    "df['time [m]'] = change_col_offset(df, 'time [m]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[0, ['initial feed concentration [g/L]', 'initial permeate concentration [g/L]', 'initial retentate concentration [g/L]']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dict = {\n",
    "    'flux 20 °C' : df['flux at 20° [L/m^2h]'],\n",
    "    'flux' : df['flux [L/m^2h]'],\n",
    "    'res'  : df['res tot [1/m]'],\n",
    "}\n",
    "y_ax_lbl = ['flux [LMH]', 'res tot [1/m]']\n",
    "title    = 'Flux and res over Time'\n",
    "plot_time_series_2_axis(x=df[x_axis[TIME_IDX]], y_series=y_dict, x_ax_lbl=x_axis[TIME_IDX], y_ax_lbl=y_ax_lbl, title=title, x_format=x_format[TIME_IDX], secondary_y=['res'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dict = {\n",
    "    'flux 20 °C' : df['flux at 20° [L/m^2h]'],\n",
    "    'flux' : df['flux [L/m^2h]'],\n",
    "    'TMP'  : df['TMP [kPa]'],\n",
    "}\n",
    "y_ax_lbl = ['flux [LMH]', 'TMP [kPa]']\n",
    "title    = 'Flux and TMP over Time'\n",
    "plot_time_series_2_axis(x=df[x_axis[TIME_IDX]], y_series=y_dict, x_ax_lbl=x_axis[TIME_IDX], y_ax_lbl=y_ax_lbl, title=title, x_format=x_format[TIME_IDX], secondary_y=['TMP'], loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(l, r) = TMP_intervals[tmp_idx] #.get(tmp_idx, (None, None))\n",
    "if l is not None :\n",
    "    df = df[(df['time [m]'] >= l)]\n",
    "if r is not None : \n",
    "    df = df[(df['time [m]'] < r)]\n",
    "df = df.reset_index(drop=True)\n",
    "df['time [m]'] = change_col_offset(df, 'time [m]')\n",
    "# USE_ARIMA_FORECAST = (len(df) > 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add estimated columns\n",
    "PRS_RETENTATE_EST_kPa = df['prs retentate [kPa]'].median()\n",
    "df['prs retentate est [kPa]'] = PRS_RETENTATE_EST_kPa\n",
    "estimated_columns = [\"prs feed_1 [kPa]\", \"prs feed_2 [kPa]\", \"prs permeate [kPa]\", \"flow retentate [L/h]\"]\n",
    "for c in estimated_columns:\n",
    "    c_est = re.sub(r'(\\[.*\\])$', r'est \\1', c)\n",
    "    print(c_est)\n",
    "    model = read_estimated_coefficients_from_json(FILE_EST_COEFFS, c)\n",
    "    cols = model['x']\n",
    "    if cols == None :\n",
    "        df[c_est] = model['intercept']\n",
    "    else :\n",
    "        df[c_est] = predict_y(df[model['x']], model['intercept'], model['coefficients']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TMP est [kPa]'] = 0.5 * (df['prs retentate est [kPa]'] + df['prs feed_2 est [kPa]']) - df['prs permeate est [kPa]']\n",
    "PRS_TMP_EST_kPA = df.loc[0,'TMP est [kPa]']\n",
    "PRS_PRMT_EST_kPA = df.loc[0,'prs permeate est [kPa]']\n",
    "print(\"estimated TMP:\", PRS_TMP_EST_kPA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_TMP_levels(df, col='TMP est [kPa]')\n",
    "TMP_LVL_COLS = [ c for c in df.columns if c[:9] == 'is TMP in']\n",
    "print(TMP_LVL_COLS)\n",
    "df[:2][ ['TMP est [kPa]'] + TMP_LVL_COLS ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['res tot [%]'] = min_max_scaler(df['res tot [1/m]'])\n",
    "df['TMP [%]'] = min_max_scaler(df['TMP [kPa]'])\n",
    "df['viscosity permeate [%]'] = min_max_scaler(df['viscosity permeate [Pa s]'])\n",
    "df['prs permeate [%]'] = min_max_scaler(df['prs permeate [kPa]'])\n",
    "y_dict = {\n",
    "    'flux 20 °C' : df['flux at 20° [L/m^2h]'],\n",
    "    'flux' : df['flux [L/m^2h]'],\n",
    "    'res'  : df['res tot [%]'],\n",
    "    'TMP'  : df['TMP [%]'],\n",
    "    'viscosity' : df['viscosity permeate [%]'],\n",
    "    #'prs permeate [%]' : df['prs permeate [%]']\n",
    "}\n",
    "print(df.agg({\n",
    "    'flux at 20° [L/m^2h]'          : [\"min\", \"median\", \"mean\",  \"max\", \"std\", \"var\"],\n",
    "    'flux [L/m^2h]'                 : [\"min\", \"median\", \"mean\",  \"max\", \"std\", \"var\"],\n",
    "    \"res tot [1/m]\"                 : [\"min\", \"median\", \"mean\",  \"max\", \"std\", \"var\"],\n",
    "    \"TMP [kPa]\"                     : [\"min\", \"median\", \"mean\",  \"max\", \"std\", \"var\"],\n",
    "    \"viscosity permeate [Pa s]\"     : [\"min\", \"median\", \"mean\",  \"max\", \"std\", \"var\"],\n",
    "    #\"prs permeate [kPa]\"            : [\"min\", \"median\", \"mean\",  \"max\", \"std\", \"var\"],\n",
    "}))\n",
    "\n",
    "y_ax_lbl = ['flux [LMH]', 'factor [%]']\n",
    "title    = 'Flux vs its factors over time'\n",
    "#plot_time_series_1_axis(x=df[x_axis[TIME_IDX]], y_series=y_dict, x_ax_lbl=x_axis[TIME_IDX], y_ax_lbl=y_ax_lbl, title=title, x_format=x_format[TIME_IDX])\n",
    "plot_time_series_2_axis(x=df[x_axis[TIME_IDX]], y_series=y_dict, x_ax_lbl=x_axis[TIME_IDX], y_ax_lbl=y_ax_lbl, title=title, x_format=x_format[TIME_IDX], secondary_y=['res', 'TMP', 'viscosity'], loc='upper center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_bckup is not None :\n",
    "    df = df_bckup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(FLUX_COL)\n",
    "#df[['flux at 20° [L/m^2h]', 'flux [L/m^2h]']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACF  plot: autocorrelations which measure the relationship between y_t and y_{t-k} for k >= 1\n",
    "# PACF plot: autocorrelations which measure the relationship between y_t and y_{t-k} for k >= 1 AFTER removing the effects of i=1,...,k-1\n",
    "# max_lags < n/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the data are from an ARIMA(p, d, 0) or ARIMA(0, d, q) model, then the ACF and PACF plots can be helpful in determining the value of p or q.\n",
    "#If p and q are both positive, then the plots do not help in finding suitable values of p and q.\n",
    "#The data may follow an ARIMA(p, d, 0) model if the ACF and PACF plots of the differenced data show the following patterns:\n",
    "# - the ACF is exponentially decaying or sinusoidal;\n",
    "# - there is a significant spike at lag p in the PACF, but none beyond lag p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[FLUX_COL]\n",
    "adf_test, is_significant = check_stationarity(df_train, lags='default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "differencing = True if use_default_arima_params else (not is_significant)\n",
    "\n",
    "if not differencing :\n",
    "    print(\"NOT differencing\")\n",
    "else :\n",
    "    ok = False\n",
    "    df_train_diff = df_train\n",
    "    d = 0\n",
    "    while not ok and d <= 0:\n",
    "        d += 1\n",
    "        print(f\"differencing d={d}\")\n",
    "        df_train_diff = df_train_diff.diff().dropna()\n",
    "        adf_test, ok = check_stationarity(df_train_diff, lags='default', show_plots=True)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d if differencing else 0\n",
    "print(f\"d: {d}\")\n",
    "auto_arima = pm.auto_arima(df_train, stepwise=False, seasonal=False, trace=True, information_criterion='hqic', d=d)\n",
    "(p,d,q) = auto_arima.get_params()['order']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_default_arima_params :\n",
    "    # override auto_arima best choice\n",
    "    (p,d,q) = default_arima_params\n",
    "print((p,d,q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = arima.ARIMA(df_train, order=(p,d,q))\n",
    "model_fit = model.fit()\n",
    "model_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = model_fit.forecast(1000)\n",
    "print(forecast[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df0 = df[[x_axis[TIME_IDX], FLUX_COL]][:5]\n",
    "#df0['f(time [m])'] = df0[x_axis[TIME_IDX]].apply(lambda x : x**(1/3)) #x**(0.35))\n",
    "#display(df0[:5])\n",
    "#model00, y_pred00, intercept00, coeffs00 = call_linear_model(df0[['f(time [m])']], df0[FLUX_COL], summary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOUND PARAMETERS !!!!!!!!!!!!\n",
    "#J_0  = intercept00\n",
    "J_Re = list(forecast)[-1]\n",
    "#print(f\"J_0  = {J_0:>.5f}\")\n",
    "print(f\"J_Re = {J_Re:>.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_N = 50\n",
    "n_periods = max(0, MAX_N - len(df))\n",
    "# n_periods = 200\n",
    "x_forecast = range(len(df)+1,n_periods+len(df)+1)\n",
    "y_forecast = forecast[:n_periods]\n",
    "\n",
    "up_to = min(10, max(5, len(df)//2))\n",
    "flux_all = pd.concat([df[FLUX_COL], pd.Series(y_forecast[:up_to])]).reset_index(drop=True) #pd.Series(J_0), \n",
    "time_all = list(range(len(flux_all)))\n",
    "pct = 0.6\n",
    "flux_all_smooth = smooth_data_lowess(time_all, flux_all, pct=pct)\n",
    "print(f\"pct: {pct}, number of forecasted points used for smoothing: {up_to}\")\n",
    "compute_error_metrics(flux_all_smooth, flux_all)\n",
    "\n",
    "plot_time_series_1_axis(x=time_all,              y_series={f\"flux smoothed {pct}\" : flux_all_smooth}, x_ax_lbl=x_axis[TIME_IDX], y_ax_lbl=FLUX_COL, title=None, x_format=x_format[TIME_IDX], color=\"yellow\", figsize=(16,9))\n",
    "# plot_time_series_1_axis(x=[0],                  y_series={\"flux in t=0\" : [J_0]},                  x_ax_lbl=x_axis[TIME_IDX], y_ax_lbl=FLUX_COL, title=None, x_format=x_format[TIME_IDX], color=\"green\", figsize=None)\n",
    "plot_time_series_1_axis(x=df[x_axis[TIME_IDX]], y_series={'flux real' : df[FLUX_COL]},             x_ax_lbl=x_axis[TIME_IDX], y_ax_lbl=FLUX_COL, title=None, x_format=x_format[TIME_IDX], color=\"blue\", figsize=None)\n",
    "plot_time_series_1_axis(x=x_forecast,           y_series={'flux forecast': y_forecast},            x_ax_lbl=x_axis[TIME_IDX], y_ax_lbl=FLUX_COL, title='Flux forecast', x_format=x_format[TIME_IDX], color='red', figsize=None)\n",
    "last_t = df.loc[len(df)-1,'time [m]']\n",
    "plot.plot([last_t,last_t+1], [df.loc[len(df)-1,FLUX_COL], list(forecast)[0]], linestyle='--', color='red')\n",
    "# plot.plot([0,1], [J_0,df.loc[0, FLUX_COL]], linestyle='--', color='green')\n",
    "plot.axhline(J_Re, label='flux asymptote', linestyle='--', color='black')\n",
    "plot.grid()\n",
    "plot.legend()\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vis, y_pred_vis, intercept_vis, coeffs_vis = call_linear_model(df[['time [m]']], df['viscosity permeate [Pa s]'], summary=True)\n",
    "y_dict = {\n",
    "    'y real'  : df['viscosity permeate [Pa s]'],\n",
    "    'y pred'  : y_pred_vis,\n",
    "}\n",
    "plot_time_series_1_axis(x=df[x_axis[0]], y_series=y_dict, x_ax_lbl=x_axis[0], y_ax_lbl='viscosity [Pa s]', title='viscosity linear prediction', x_format=x_format[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since the real concentrations are known only in t=0 of the first TMP (for the others is NaN)\n",
    "# in the following TMP used the real concentration will be unknown\n",
    "# so we use the estimated concentration as the initial one\n",
    "# NB for the first TMP: conc_real = conc_est\n",
    "#df['retentate concentration [g/L]'] = df['retentate concentration est [g/L] v2']\n",
    "#df['permeate concentration [g/L]']  = df['permeate concentration est [g/L] v2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################\n",
    "# since we have few data points we use the forecast as dataset itself\n",
    "if df_bckup is None :\n",
    "    df_bckup = df\n",
    "else :\n",
    "    df = df_bckup\n",
    "df['is forecast'] = 0\n",
    "df = df[[\n",
    "    'time [m]', 'flux [L/m^2h]', 'flux at 20° [L/m^2h]', 'area tot [m^2]', 'is forecast', \n",
    "    #'retentate concentration [g/L]', 'permeate concentration [g/L]',\n",
    "]].copy() #'res tot [1/m]', 'TMP [kPa]'\n",
    "\n",
    "if include_arima_simulations_in_analysis:\n",
    "    df_forecast = pd.DataFrame()\n",
    "    df_forecast['time [m]'] = [ t for t in x_forecast]\n",
    "    df_forecast[FLUX_COL] = y_forecast.reset_index(drop=True)\n",
    "    #df_start = pd.DataFrame()\n",
    "    #df_start['time [m]'] = [ 0 ]\n",
    "    #df_start[FLUX_COL] = [ J_0 ]\n",
    "    for df_concat in [df_forecast] : #df_start, \n",
    "        df_concat['is forecast'] = 1\n",
    "        df_concat['area tot [m^2]'] = df.loc[0, 'area tot [m^2]']\n",
    "    df = pd.concat([df, df_forecast]).reset_index(drop=True) #df_start,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in TMP_LVL_COLS :\n",
    "    df[c] = df_bckup.loc[0, c] \n",
    "df['initial feed concentration [g/L]']           = df_bckup.loc[0, 'initial feed concentration [g/L]'] \n",
    "df['initial retentate concentration [g/L]']      = df_bckup.loc[0, 'initial retentate concentration [g/L]'] \n",
    "df['initial permeate concentration [g/L]']       = df_bckup.loc[0, 'initial permeate concentration [g/L]'] \n",
    "df['increased TMP'] = 0\n",
    "df['decreased TMP'] = 0\n",
    "df.loc[0, 'increased TMP'] = df_bckup.loc[0, 'increased TMP']\n",
    "df.loc[0, 'decreased TMP'] = df_bckup.loc[0, 'decreased TMP']\n",
    "df['TMP est [kPa]'] = PRS_TMP_EST_kPA\n",
    "df['viscosity permeate [Pa s]'] = predict_y(df['time [m]'], intercept_vis, coeffs_vis)\n",
    "df['res tot est [1/m]'] = df['TMP est [kPa]'] / (df['flux [L/m^2h]'] / (1000.0) * df['viscosity permeate [Pa s]'] / (1000.0 * 3600.0) )\n",
    "df['res tot est at 20° [1/m]'] = df['TMP est [kPa]'] / (df[FLUX_COL] / (1000.0) * calc_viscosity(20, pressure_Pa=101325+PRS_PRMT_EST_kPA, element='Water') / (1000.0 * 3600.0) )\n",
    "df[f'd/dt {FLUX_USED}'] = df[FLUX_COL].diff() #.fillna(0)\n",
    "df[f'{FLUX_USED} - {FLUX_USED} min [L/m^2h]'] = df[FLUX_COL] - J_Re\n",
    "df[f'{FLUX_USED} min [L/m^2h]'] = J_Re\n",
    "df[f'is {FLUX_USED} almost steady'] = df[f'd/dt {FLUX_USED}'].apply(lambda x : abs(x) < 2).astype(int)\n",
    "df[f'is {FLUX_USED} steady'] = df[f'{FLUX_USED} - {FLUX_USED} min [L/m^2h]'].apply(lambda x : abs(x) < 0.1).astype(int)\n",
    "df['date'] = DATE\n",
    "df['file_idx'] = file_idx\n",
    "df['tmp_idx'] = tmp_idx\n",
    "flux_all_smooth_lbl = f'{FLUX_USED} smoothed pct={pct}'\n",
    "df[flux_all_smooth_lbl] = pd.concat([pd.Series(flux_all_smooth), y_forecast.loc[len(flux_all_smooth):]])\n",
    "df[f'd/dt {flux_all_smooth_lbl}'] = df[flux_all_smooth_lbl].diff()\n",
    "df[f'is {flux_all_smooth_lbl} steady'] = df[flux_all_smooth_lbl].apply(lambda x : abs(x - J_Re) < 0.1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = add_resistance_smooth_and_percentages(df, res_col='res tot est [1/m]', out_cols=('res tot est smooth [1/m]', 'res tot est [%]', 'd/dt res tot est [%]'))\n",
    "#dt_res_col='d/dt res tot est [%]'\n",
    "# predict concentration in t=0 (backward pass)\n",
    "#for sample_type in ['retentate', 'permeate'] :\n",
    "#    conc_real  = f'{sample_type} concentration [g/L]'\n",
    "#    df.loc[0, conc_real] = df.loc[1, conc_real] / (1 + (initial_coeffs[sample_type] * df.loc[i, dt_res_col]))\n",
    "# predict concentration for future forecast\n",
    "#df = predict_concentration_given_coeff(df, initial_coeffs, dt_res_col=dt_res_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k_n = - d/dt flux / (flux - flux_min)\n",
    "# if flux == flux_min (in steady conditions) ==> k_n = 0/0 = NaN \n",
    "# but in steady conditions: flux(t) = flux(t-1) => k_n useless any value is ok \n",
    "# ==> we mantain the last value constant from there point on so it is easy to predict (function is continous),\n",
    "# unless LAST_K < 0, in that case we use zero (easier prediction)\n",
    "#df['flux previous [L/m^2h]'] = df[FLUX_COL].shift(-1)\n",
    "\n",
    "i = 0\n",
    "for n in ALL_N :\n",
    "    MAX_K_N = ALL_MAX_K_N[i]\n",
    "    k_n = f'k(n={n})'    \n",
    "    # calculate k_n flux slope\n",
    "    # df[k_n + ' smooth'] = df.apply(lambda x : 0.0 if x[f'is {flux_all_smooth_lbl} steady'] == 1 else fun_k_n(x[flux_all_smooth_lbl], x[f'd/dt {flux_all_smooth_lbl}'], J_Re, n), axis = 1)\n",
    "    df[k_n] = df.apply(lambda x : np.nan if x[f'is {FLUX_USED} steady'] == 1 else fun_k_n(x[FLUX_COL], x[f'd/dt {FLUX_USED}'], J_Re, n), axis = 1)\n",
    "    # add boundaries: -2 <= k_n <= 2 \n",
    "    df[k_n] = df.apply(lambda x : max(-MAX_K_N, min(MAX_K_N, x[k_n])) if pd.notna(x[k_n]) else np.nan, axis = 1)\n",
    "    # use last_k as constant once flux is steady\n",
    "\n",
    "    \n",
    "    # LAST_K = 0\n",
    "    #LAST_K = max(0, list(df[df['is flux steady'] == 0][k_n])[-1])\n",
    "    #df[k_n] = df.apply(lambda x : LAST_K if x[f'is {FLUX_USED} steady'] == 1 else x[k_n], axis = 1)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoothly update k_n towards 0\n",
    "#almost_steady_idxs = df[(df[f'is {FLUX_USED} almost steady'] == 1) & (df[f'is {FLUX_USED} steady'] == 0) & (df['is forecast'] == 1)].index\n",
    "#start = True\n",
    "#for i in almost_steady_idxs :\n",
    "#    if not start :\n",
    "#        for n in ALL_N :     \n",
    "#            k_n = f'k(n={n})'\n",
    "#            new_k = df.loc[i-1, k_n] / 1.1\n",
    "#            df.loc[i, k_n] = 0 if new_k < 1e-8 else new_k\n",
    "#    start = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[-10:][[FLUX_COL, f'd/dt {FLUX_USED}',  f'{FLUX_USED} min [L/m^2h]', 'is forecast', f'is {FLUX_USED} steady', 'k(n=0)', 'k(n=1)', 'k(n=1.5)', 'k(n=2)']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_real  = df[df['is forecast'] == 0]\n",
    "df_frcst = df[df['is forecast'] == 1]\n",
    "\n",
    "rows = 2\n",
    "cols = 2\n",
    "fig, axs = plot.subplots(rows, cols, figsize=(16,12))\n",
    "plot.title('k(n) = - flux\\' / (flux - flux_min)')\n",
    "i = 0\n",
    "j = 0\n",
    "for n in ALL_N :\n",
    "    k_n = f'k(n={n})'\n",
    "    median_val = df_real[k_n].median()\n",
    "    mean_val = df_real[k_n].mean()\n",
    "    cur_ax = axs[j] if rows == 1 else axs[i, j]\n",
    "    #cur_ax.plot(df[x_axis[TIME_IDX]],  df[k_n],  color='green', label=k_n + ' real')\n",
    "    #cur_ax.plot(df[x_axis[TIME_IDX]],  df[k_n + ' smooth'],  color='gold', label=k_n + ' smooth')\n",
    "    cur_ax.plot(df_real[x_axis[TIME_IDX]],  df_real[k_n],  color='green', label=k_n + ' real')\n",
    "    cur_ax.plot(df_frcst[x_axis[TIME_IDX]], df_frcst[k_n], color='red',  label=k_n + ' forecast')\n",
    "    #cur_ax.axhline(median_val, label=f'{k_n} median', linestyle='--', color='black')\n",
    "    #cur_ax.axhline(mean_val, label=f'{k_n} mean', linestyle='--', color='blue')\n",
    "    cur_ax.set_xlabel(x_axis[TIME_IDX])\n",
    "    cur_ax.set_ylabel(k_n)\n",
    "    cur_ax.set_title(k_n)\n",
    "    cur_ax.legend()\n",
    "    cur_ax.grid()\n",
    "    j = (j + 1) % cols\n",
    "    i = (i + 1) if (j == 0) else i\n",
    "\n",
    "#plot_time_series_1_axis(x=df_real[x_axis[TIME_IDX]],  y_series={'k(n=2) real' :    df_real['k(n=2)']}, x_ax_lbl=x_axis[TIME_IDX], y_ax_lbl=None, title=None, x_format=x_format[TIME_IDX], color=['green'])\n",
    "#plot_time_series_1_axis(x=df_frcst[x_axis[TIME_IDX]], y_series={'k(n=2) forecast': df_frcst['k(n=2)']}, x_ax_lbl=x_axis[TIME_IDX], y_ax_lbl='k(n=2)', title='k(n=2) = - flux\\' / (flux - flux_min)', x_format=x_format[TIME_IDX], color=['gold'], figsize=None)\n",
    "#plot.legend()\n",
    "#plot.grid()\n",
    "#plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(get_summary(df_real, [f'k(n={n})' for n in ALL_N], group_cols=[], stats=[\"min\", \"median\", \"mean\",  \"max\", \"var\"], transpose='default', conf_int=0.95))\n",
    "#for n in ALL_N :\n",
    "#    print(get_error_stats(df_real[f'k(n={n})'], df_real[f'k(n={n})'].median(), f'k(n={n})'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_k_n = pd.DataFrame() # WARNING: clears all csv data!\n",
    "# remove current df if is present\n",
    "if not df_k_n.empty :\n",
    "    df_k_n = df_k_n[ ((df_k_n['date'] != DATE) | (df_k_n['file_idx'] != file_idx) | (df_k_n['tmp_idx'] != tmp_idx)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_separator_row = {\n",
    "    'file_idx' : file_idx,\n",
    "    'tmp_idx'  : tmp_idx,\n",
    "    'date'     : DATE,\n",
    "    #'time [m]' : -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the current row to the df\n",
    "df_k_n = pd.concat([df_k_n, df], ignore_index=True).reset_index(drop=True)\n",
    "df_k_n.loc[len(df_k_n)] = empty_separator_row\n",
    "df_k_n = df_k_n.sort_values(by=['date', 'file_idx', 'tmp_idx', 'time [m]']).reset_index(drop=True)\n",
    "df_k_n = df_k_n[[\n",
    "    'file_idx', 'tmp_idx', 'date', 'time [m]', 'is forecast', \n",
    "    'flux [L/m^2h]', 'flux at 20° [L/m^2h]',\n",
    "    f'd/dt {FLUX_USED}', f'{FLUX_USED} min [L/m^2h]', \n",
    "    'res tot est [1/m]',  'res tot est at 20° [1/m]',\n",
    "    'TMP est [kPa]',  'increased TMP', 'decreased TMP',\n",
    "    'viscosity permeate [Pa s]', \n",
    "    'k(n=0)', 'k(n=1)', 'k(n=1.5)', 'k(n=2)', f'is {FLUX_USED} steady', #'k(n=0) smooth', 'k(n=1) smooth', 'k(n=1.5) smooth', 'k(n=2) smooth', \n",
    "    #'res tot est smooth [1/m]', 'res tot est [%]', 'd/dt res tot est [%]',\n",
    "    'initial feed concentration [g/L]', 'initial retentate concentration [g/L]', 'initial permeate concentration [g/L]'\n",
    "    #'retentate concentration est [g/L]', 'permeate concentration est [g/L]'\n",
    "] + TMP_LVL_COLS ]\n",
    "df_k_n.to_csv(out_file, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
